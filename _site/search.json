[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Understanding Feature Importance in Machine Learning",
    "section": "",
    "text": "In the fast-evolving field of machine learning, understanding how models make predictions is just as important as achieving high accuracy. This is where feature importance becomes a critical tool. Feature importance answers questions like:\n\nWhich features contribute most to a model’s predictions?\nHow can we ensure our models are interpretable and fair?\nWhat insights can we extract about the data itself?\n\nAs a machine learning enthusiast, I vividly remember the class where we explored feature importance. It was an eye-opening session that demonstrated how interpretability can enhance the trustworthiness and effectiveness of machine learning systems. This blog dives into the concept of feature importance, illustrating its application with the California housing dataset."
  },
  {
    "objectID": "index.html#correlation-heatmap",
    "href": "index.html#correlation-heatmap",
    "title": "Understanding Feature Importance in Machine Learning",
    "section": "3.1 1. Correlation Heatmap",
    "text": "3.1 1. Correlation Heatmap\nBefore diving into feature importance, I visualized the feature correlations to understand relationships in the data. The heatmap below reveals some interesting patterns:\n\nAveRooms and AveBedrms are strongly correlated (r = 0.85), indicating potential redundancy.\nGeographic features like Latitude and Longitude are also highly related, which is expected given the dataset’s regional nature.\n\n\n\n\nFeature Correlation Heatmap\n\n\nUnderstanding these correlations is crucial, as it helps identify multicollinearity, which can influence model interpretation."
  },
  {
    "objectID": "index.html#feature-importance-with-random-forest",
    "href": "index.html#feature-importance-with-random-forest",
    "title": "Understanding Feature Importance in Machine Learning",
    "section": "3.2 2. Feature Importance with Random Forest",
    "text": "3.2 2. Feature Importance with Random Forest\nNext, I trained a Random Forest model and computed feature importance scores. Random Forest is a versatile algorithm that naturally provides feature importance based on tree splits. The bar chart below highlights the most influential features:\n\nMedInc (Median Income) dominates as the most important feature, which makes sense since income levels often correlate strongly with housing prices.\nAveOccup (average occupants per household) and Latitude also show significant influence.\n\n\n\n\nRandom Forest Feature Importance plot highlighting MedInc as the most influential feature, followed by AveOccup and Latitude.\n\n\nThis global view of feature importance provides a solid understanding of which features matter most to the model."
  },
  {
    "objectID": "index.html#shap-a-deeper-dive-into-explainability",
    "href": "index.html#shap-a-deeper-dive-into-explainability",
    "title": "Understanding Feature Importance in Machine Learning",
    "section": "3.3 3. SHAP: A Deeper Dive Into Explainability",
    "text": "3.3 3. SHAP: A Deeper Dive Into Explainability\nSHAP (SHapley Additive exPlanations) takes feature importance a step further by quantifying each feature’s contribution to individual predictions. This local interpretability is invaluable for understanding edge cases.\nThe SHAP summary plot below illustrates:\n\nHigh MedInc values positively influence predictions, significantly increasing housing prices.\nFeatures like Latitude and AveOccup also play key roles but show varying impacts depending on their values.\n\n\n\n\nSHAP Summary Plot showing the impact of features on housing price predictions. Higher MedInc values positively influence predictions, while Latitude and AveOccup have varying effects depending on their values.\n\n\nSHAP allows us to zoom into individual predictions, making it an excellent tool for debugging and fairness analysis.\nSHAP Dependence Plot\nTo understand feature interactions, I plotted a SHAP dependence plot below which highlights how MedInc strongly influences predictions. The SHAP values for MedInc increase with its value, showing its positive impact on housing price predictions. The color gradient represents AveOccup, revealing an additional interaction where higher AveOccup values slightly modulate the effect of MedInc.\n\n\n\nSHAP Dependence Plot showing the interaction between Median Income (MedInc) and Latitude. MedInc positively impacts predictions, with AveOccup slightly modulating its effect.\n\n\nSHAP Waterfall Plot\nFinally, I used a SHAP waterfall plot to break down the prediction for a single data point. This plot shows how each feature’s SHAP value contributes to the final prediction:\nMedInc contributes the most, followed by smaller contributions from Latitude and AveOccup.\nThe plot provides a cumulative view of feature effects, offering a detailed explanation of the model’s decision\n\n\n\nSHAP Waterfall Plot illustrating how individual feature contributions (like MedInc and Latitude) combine to predict housing prices for a single instance(this case first datapoint)."
  },
  {
    "objectID": "images/Untitled.html",
    "href": "images/Untitled.html",
    "title": "Yas Blog",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load Titanic dataset\nurl = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\ntitanic = pd.read_csv(url)\n\n# Select relevant features and preprocess\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex', 'Embarked']\ntitanic = titanic.dropna(subset=['Age', 'Embarked'])\ntitanic['Sex'] = titanic['Sex'].map({'male': 0, 'female': 1})\ntitanic['Embarked'] = titanic['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\nX = titanic[features]\ny = titanic['Survived']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nSex\nEmbarked\n\n\n\n\n472\n2\n33.0\n1\n2\n27.7500\n1\n2\n\n\n432\n2\n42.0\n1\n0\n26.0000\n1\n2\n\n\n666\n2\n25.0\n0\n0\n13.0000\n0\n2\n\n\n30\n1\n40.0\n0\n0\n27.7208\n0\n0\n\n\n291\n1\n19.0\n1\n0\n91.0792\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n93\n3\n26.0\n1\n2\n20.5750\n0\n2\n\n\n135\n2\n23.0\n0\n0\n15.0458\n0\n0\n\n\n338\n3\n45.0\n0\n0\n8.0500\n0\n2\n\n\n549\n2\n8.0\n1\n1\n36.7500\n0\n2\n\n\n131\n3\n20.0\n0\n0\n7.0500\n0\n2\n\n\n\n\n569 rows × 7 columns\n\n\n\n\n# Plot heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(X_train.corr(), annot=True, cmap='coolwarm')\nplt.title('Feature Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Train Random Forest and compute feature importance\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\nimportances = model.feature_importances_\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(X.columns, importances)\nplt.title('Feature Importance (Random Forest)')\nplt.xlabel('Importance Score')\nplt.ylabel('Feature')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_train)\n\n# Summary Plot\nshap.summary_plot(shap_values[1], X_train, feature_names=X_train.columns)\n\n# Dependence Plot for 'Fare'\nif 'Fare' in X_train.columns:\n    shap.dependence_plot('Fare', shap_values[1], X_train, feature_names=X_train.columns)\nelse:\n    print(\"Feature 'Fare' is not in the dataset.\")\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[2], line 5\n      2 shap_values = explainer.shap_values(X_train)\n      4 # Summary Plot\n----&gt; 5 shap.summary_plot(shap_values[1], X_train, feature_names=X_train.columns)\n      7 # Dependence Plot for 'Fare'\n      8 if 'Fare' in X_train.columns:\n\nFile ~/miniforge3/envs/573/lib/python3.12/site-packages/shap/plots/_beeswarm.py:543, in summary_legacy(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, show_values_in_legend, use_log_scale)\n    540         assert False, shape_msg + \" Perhaps the extra column in the shap_values matrix is the \" \\\n    541                       \"constant offset? Of so just pass shap_values[:,:-1].\"\n    542     else:\n--&gt; 543         assert num_features == features.shape[1], shape_msg\n    545 if feature_names is None:\n    546     feature_names = np.array([labels['FEATURE'] % str(i) for i in range(num_features)])\n\nAssertionError: The shape of the shap_values matrix does not match the shape of the provided data matrix.\n\n\n\n\nprint(f\"shap_values shape: {shap_values[1].shape}\")\nprint(f\"X_train shape: {X_train.shape}\")\n\nshap_values shape: (7, 2)\nX_train shape: (569, 7)\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport shap\nimport matplotlib.pyplot as plt\n\n# Load Titanic dataset\nurl = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\ntitanic = pd.read_csv(url)\n\n# Preprocessing\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex', 'Embarked']\ntarget = 'Survived'\ntitanic = titanic.dropna(subset=['Age', 'Embarked'])\nX = titanic[features]\ny = titanic[target]\n\n# Preprocessor: handle numeric and categorical features\nnumeric_features = ['Age', 'Fare', 'Parch', 'SibSp']\ncategorical_features = ['Pclass', 'Sex', 'Embarked']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ]\n)\n\n# Create pipeline with Random Forest\nmodel_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model\nmodel_pipeline.fit(X_train, y_train)\n\n# Transform the test data for SHAP explanation\nX_test_enc = pd.DataFrame(\n    data=preprocessor.transform(X_test),\n    columns=(numeric_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out())),\n    index=X_test.index\n)\n\n# Compute SHAP values\nexplainer = shap.TreeExplainer(model_pipeline.named_steps['classifier'])\nshap_values = explainer.shap_values(X_test_enc)\n\n# SHAP summary plot for the positive class (class 1)\nshap.summary_plot(shap_values[1], X_test_enc)\n\n# SHAP dependence plot for 'Fare'\nshap.dependence_plot('Fare', shap_values[1], X_test_enc)\n\n# Choose a specific example for a waterfall plot\nexample_index = 10  # Example index for explanation\nshap.initjs()  # Load JS for visualization\nshap.plots.waterfall(\n    shap.Explanation(\n        values=shap_values[1][example_index],\n        base_values=explainer.expected_value[1],\n        data=X_test_enc.iloc[example_index].values,\n        feature_names=X_test_enc.columns\n    )\n)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[4], line 56\n     53 shap_values = explainer.shap_values(X_test_enc)\n     55 # SHAP summary plot for the positive class (class 1)\n---&gt; 56 shap.summary_plot(shap_values[1], X_test_enc)\n     58 # SHAP dependence plot for 'Fare'\n     59 shap.dependence_plot('Fare', shap_values[1], X_test_enc)\n\nFile ~/miniforge3/envs/573/lib/python3.12/site-packages/shap/plots/_beeswarm.py:543, in summary_legacy(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, show_values_in_legend, use_log_scale)\n    540         assert False, shape_msg + \" Perhaps the extra column in the shap_values matrix is the \" \\\n    541                       \"constant offset? Of so just pass shap_values[:,:-1].\"\n    542     else:\n--&gt; 543         assert num_features == features.shape[1], shape_msg\n    545 if feature_names is None:\n    546     feature_names = np.array([labels['FEATURE'] % str(i) for i in range(num_features)])\n\nAssertionError: The shape of the shap_values matrix does not match the shape of the provided data matrix.\n\n\n\n\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\n# Load the California Housing dataset\ndata = fetch_california_housing(as_frame=True)\ndf = data.frame\nfeatures = data.feature_names\ntarget = \"MedHouseVal\"\n\nprint(df.head())\n\n   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n\n   Longitude  MedHouseVal  \n0    -122.23        4.526  \n1    -122.22        3.585  \n2    -122.24        3.521  \n3    -122.25        3.413  \n4    -122.25        3.422  \n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nX = df[features]\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute the correlation matrix\ncorr_matrix = X.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.4)\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()\nplt.savefig(\"correlation_heatmap.png\")\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Train a Random Forest model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(random_state=42) \n\n\n\n# Plot feature importance\nimportances = model.feature_importances_\nfeature_names = X.columns\n\nplt.figure(figsize=(10, 6))\nplt.barh(feature_names, importances)\nplt.title(\"Feature Importance (Random Forest)\")\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Feature\")\nplt.show()\nplt.savefig(\"feature_importance.png\")\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nimport shap\nimport matplotlib.pyplot as plt\n\n# Reduce the size of the test dataset to 100 samples\nX_test_small = X_test.sample(n=100, random_state=42)\n\n# Initialize the SHAP explainer\nexplainer = shap.TreeExplainer(model)\n\n# Compute SHAP values for the smaller dataset\n# shap_values = explainer.shap_values(X_test_small)\n\n\n# # Optional: Dependence plot for a specific feature (e.g., MedInc)\n# shap.dependence_plot(\"MedInc\", shap_values, X_test_small)\n\n# plt.savefig(\"dependence.png\")\n\n\n# Generate SHAP values for the small dataset\nshap_values = explainer.shap_values(X_test_small)\n\n# Dependence plot for MedInc with Latitude interaction\nshap.dependence_plot(\"MedInc\", shap_values[0], X_test_small, interaction_index=\"Latitude\")\nplt.savefig(\"dependence.png\")\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[11], line 24\n     21 shap_values = explainer.shap_values(X_test_small)\n     23 # Dependence plot for MedInc with Latitude interaction\n---&gt; 24 shap.dependence_plot(\"MedInc\", shap_values[0], X_test_small, interaction_index=\"Latitude\")\n     25 plt.savefig(\"dependence.png\")\n\nFile ~/miniforge3/envs/dsci572/lib/python3.12/site-packages/shap/plots/_scatter.py:612, in dependence_legacy(ind, shap_values, features, feature_names, display_features, interaction_index, color, axis_color, cmap, dot_size, x_jitter, alpha, title, xmin, xmax, ax, show, ymin, ymax)\n    609         pl.show()\n    610     return\n--&gt; 612 assert shap_values.shape[0] == features.shape[0], \\\n    613     \"'shap_values' and 'features' values must have the same number of rows!\"\n    614 assert shap_values.shape[1] == features.shape[1], \\\n    615     \"'shap_values' must have the same number of columns as 'features'!\"\n    617 # get both the raw and display feature values\n\nAssertionError: 'shap_values' and 'features' values must have the same number of rows!\n\n\n\n\n\n\n\n\n\n\n\nimport shap\n\n# Reduce the test dataset size for SHAP calculations\nX_test_small = X_test.sample(n=100, random_state=42)\n\n# Initialize the SHAP explainer\nexplainer = shap.TreeExplainer(model)\n\n# Compute SHAP values for the smaller dataset\nshap_values = explainer(X_test_small)  # Use the new SHAP API to get Explanation object\n\n# Waterfall plot for a single prediction (e.g., the first data point)\nshap.plots.waterfall(shap_values[0])\nplt.savefig(\"waterfall.png\")\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nplt.savefig(\"correlation_heatmap.png\")\nplt.savefig(\"feature_importance.png\")\nshap.summary_plot(shap_values, X_test_small, show=False)\nplt.savefig(\"shap_summary.png\")"
  }
]